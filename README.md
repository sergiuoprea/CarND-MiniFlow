# CarND-MiniFlow

## Getting started

The main purpose of this repository and as part fo the Self-Driving Car Nanodegree we will build our own library called Miniflow. This library will be our own version of TensorFlow! Following the implementation, we pretend to demystify two concepts of neural networks such as: __backpropagation__ and __differentiable graphs__.

We already know that __backpropagation__ is the process by which neural networks update the weights of the network over time. At the same time, __differentiable graphs__ are graphs where nodes are differentiable function. A differentiable function is a function whose derivative exists at each point in its domain. This is, there is a tangent line at each point in its domain which will indicate us the slope at each timestep and the direction of the __gradient descent__ process. The main purpose of __gradient descent__ which is a first-order iterative optimization algorithm is to find the minimum of a function. The purpose of this work is not to define these concepts in detail, so at the moment we should focus only on its brief definition. 

## Author

**Sergiu Ovidiu Oprea**- *PhD student at University of Alicante*

## License

This project is licensed under the MIT License - see the [LICENSE.md](https://github.com/sergiuoprea/CarND-MiniFlow/blob/master/LICENSE)

## Acknowledgments

This repository is based on the work done for the [Self-Driving Car Nanodegree](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013), Online at Udacity. **Please respect [Udacity Honor Code!](https://www.udacity.com/legal/community-guidelines)**.
